{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\n",
    "# pyspark读写dataframe\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 创建SparkSession\n",
    "spark=SparkSession.builder.appName('Titanic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取训练集，带有header，自动推断字段类型\n",
    "df = spark.read.csv(\"./train.csv\", header=True, inferSchema=True).cache()\n",
    "# 创建临时表train\n",
    "df.createOrReplaceTempView(\"train\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## EDA探索\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "# 输出schema，dataframe的数据结构信息\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|               714|\n",
      "|   mean| 29.69911764705882|\n",
      "| stddev|14.526497332334035|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对Age字段进行描述统计\n",
    "df.describe(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 缺失值统计, Spark SQL类型转换使用cast, col函数将字符串转换为column对象\n",
    "df.select(*(\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   30|\n",
      "|    null|    2|\n",
      "|       C|   93|\n",
      "|       S|  217|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用\"\"\"定义多行字符串\n",
    "query = \"\"\"\n",
    "SELECT Embarked, count(PassengerId) as count\n",
    "FROM train\n",
    "WHERE Survived = 1\n",
    "GROUP BY Embarked\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age字段缺失个数（处理前）: 177\n",
      "Age字段缺失个数（处理后）: 0\n"
     ]
    }
   ],
   "source": [
    "# ## 数据预处理\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "# cabin字段缺失值多，去掉该字段\n",
    "df = df.drop('cabin')\n",
    "before = df.select('age').where('age is null').count()\n",
    "print(\"Age字段缺失个数（处理前）: {}\".format(before))\n",
    "# 使用df.na处理缺失值\n",
    "test = df.na.drop(subset=[\"age\"])\n",
    "after = test.select('age').where('age is null').count()\n",
    "print(\"Age字段缺失个数（处理后）: {}\".format(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|survived|count|\n",
      "+--------+-----+\n",
      "|       1|  160|\n",
      "|       0|   63|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照survived字段统计\n",
    "df.groupBy('survived').count().show()\n",
    "\n",
    "# 按照survived的一定比例进行采样\n",
    "sample_df = df.sampleBy('survived', fractions={0: 0.1, 1: 0.5}, seed=0)\n",
    "sample_df.groupBy('survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|len_name|\n",
      "+--------------------+--------+\n",
      "|Braund, Mr. Owen ...|      23|\n",
      "|Cumings, Mrs. Joh...|      51|\n",
      "|Heikkinen, Miss. ...|      22|\n",
      "|Futrelle, Mrs. Ja...|      44|\n",
      "|Allen, Mr. Willia...|      24|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 添加字段len_name，代表 乘客name的长度\n",
    "str_length = udf(lambda x: len(x), IntegerType())\n",
    "# 使用withColumn添加字段\n",
    "df = df.withColumn('len_name', str_length(df['name']))\n",
    "df.select('name', 'len_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|embarked|embarked_index|\n",
      "+--------+--------------+\n",
      "|       S|             3|\n",
      "|       C|             1|\n",
      "|       S|             3|\n",
      "|       S|             3|\n",
      "|       S|             3|\n",
      "+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将类别变量 转化为数值\n",
    "def embarked_to_int(embarked):\n",
    "    if embarked == 'C': return 1\n",
    "    if embarked == 'Q': return 2\n",
    "    if embarked == 'S': return 3    \n",
    "    return 0\n",
    "\n",
    "# 使用udf，定义函数，将类别变量 转化为数值，使用Spark ML中StringIndexer，结果也是一样的\n",
    "embarked_to_int = udf(embarked_to_int, IntegerType())\n",
    "# 添加embarked_index字段\n",
    "df = df.withColumn('embarked_index', embarked_to_int(df['embarked']))\n",
    "df.select('embarked', 'embarked_index').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PassengerId': 446.0, 'Survived': 0.3838383838383838, 'Pclass': 2.308641975308642, 'Name': None, 'Sex': None, 'Age': 29.69911764705882, 'SibSp': 0.5230078563411896, 'Parch': 0.38159371492704824, 'Ticket': 260318.54916792738, 'Fare': 32.2042079685746, 'Embarked': None, 'len_name': 27.20314253647587, 'embarked_index': 2.529741863075196}\n"
     ]
    }
   ],
   "source": [
    "# 计算各列的均值\n",
    "mean = df.agg(*(mean(c).alias(c) for c in df.columns))\n",
    "# 字典数据持久化\n",
    "meaninfo = mean.first().asDict()\n",
    "print(meaninfo)\n",
    "# 填充\n",
    "df = df.fillna(meaninfo[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   sex|sex_ix|\n",
      "+------+------+\n",
      "|  male|     0|\n",
      "|female|     1|\n",
      "|female|     1|\n",
      "|female|     1|\n",
      "|  male|     0|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将sex字段进行数值编码\n",
    "df.select('sex', \n",
    "    when(df['sex'] == 'male', 0).otherwise(1).alias('sex_ix')).show(5)\n",
    "\n",
    "\n",
    "# ## 数据抽取，转换与特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|   Sex|sex_index|\n",
      "+------+---------+\n",
      "|  male|      0.0|\n",
      "|female|      1.0|\n",
      "|female|      1.0|\n",
      "|female|      1.0|\n",
      "|  male|      0.0|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "# StringIndexer将一组字符型标签编码成一组标签索引\n",
    "df = StringIndexer(inputCol='Sex', outputCol='sex_index').fit(df).transform(df)\n",
    "df.select('Sex', 'sex_index').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将指定的多个列进行合并，方便后续的ML计算\n",
    "inputCols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'embarked_index', 'sex_index', 'len_name']\n",
    "#创建features列，使用VectorAssembler将给定列列表组合成单个向量列\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "train = assembler.transform(df).select('PassengerId', col('Survived').alias('label'), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------+\n",
      "|PassengerId|label|            features|\n",
      "+-----------+-----+--------------------+\n",
      "|          1|    0|[3.0,22.0,1.0,0.0...|\n",
      "|          2|    1|[1.0,38.0,1.0,0.0...|\n",
      "|          3|    1|[3.0,26.0,0.0,0.0...|\n",
      "|          4|    1|[1.0,35.0,1.0,0.0...|\n",
      "|          5|    0|[3.0,35.0,0.0,0.0...|\n",
      "+-----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|PassengerId|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|         12|    1|[1.0,58.0,0.0,0.0...|[3.94059419701568...|[0.19702970985078...|       1.0|\n",
      "|         22|    1|[2.0,34.0,0.0,0.0...|[17.1907170516018...|[0.85953585258009...|       0.0|\n",
      "|         50|    0|[3.0,18.0,1.0,0.0...|[11.9729748030348...|[0.59864874015174...|       0.0|\n",
      "|         51|    0|[3.0,7.0,4.0,1.0,...|[15.6924051556717...|[0.78462025778358...|       0.0|\n",
      "|         54|    1|[2.0,29.0,1.0,0.0...|[3.17866143258140...|[0.15893307162907...|       1.0|\n",
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 模型训练与预测\n",
    "\n",
    "\n",
    "# 使用随机森林\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# 将数据集切分为80%训练集，20%测试集\n",
    "splits = train.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()\n",
    "\n",
    "# cacheNodeIds: 是否缓存节点ID\n",
    "model = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    cacheNodeIds=True)\n",
    "# 使用train进行训练，test进行预测\n",
    "predict = model.fit(train).transform(test)\n",
    "predict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491620111731844\n",
      "0.8147235905856596\n"
     ]
    }
   ],
   "source": [
    "# ## 模型评估\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\"\"\"\n",
    "    二分类评估：BinaryClassificationEvaluator\n",
    "    多分类评估：MulticlassClassificationEvaluator\n",
    "    回归评估：RegressionEvaluator\n",
    "    聚类评估：ClusteringEvaluator\n",
    "\"\"\"\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", \n",
    "    labelCol=\"label\", \n",
    "    metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predict))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction',\n",
    "    labelCol='label',\n",
    "    metricName='areaUnderROC')\n",
    "print(evaluator.evaluate(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='RandomForestClassifier_4d88d0f5b91c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numTrees\n",
    "model.maxDepth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
